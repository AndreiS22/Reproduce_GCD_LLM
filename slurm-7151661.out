Running SLURM prolog script on indigo54.cluster.local
===============================================================================
Job started on Sun Mar  9 01:23:08 GMT 2025
Job ID          : 7151661
Job name        : GCD_train.sh
WorkDir         : /mainfs/home/as1g21/GCD
Command         : /mainfs/home/as1g21/GCD/GCD_train.sh
Partition       : gpu
Num hosts       : 1
Num cores       : 32
Num of tasks    : 1
Hosts allocated : indigo54
Job Output Follows ...
===============================================================================
SLURM job: True
0 - SLURM_JOB_ID: 7151661
0 - SLURM_JOB_NODELIST: indigo54
0 - SLURM_JOB_NUM_NODES: 1
0 - SLURM_NTASKS: None
0 - SLURM_TASKS_PER_NODE: 1
0 - SLURM_MEM_PER_NODE: 32768
0 - SLURM_MEM_PER_CPU: None
0 - SLURM_NODEID: 0
0 - SLURM_PROCID: 0
0 - SLURM_LOCALID: 0
0 - SLURM_TASK_PID: 99013
Traceback (most recent call last):
  File "/mainfs/home/as1g21/GCD/train.py", line 291, in <module>
    main(params)
  File "/mainfs/home/as1g21/GCD/train.py", line 196, in main
    init_distributed_mode(params)
  File "/mainfs/home/as1g21/GCD/src/slurm.py", line 88, in init_distributed_mode
    params.world_size = int(os.environ['SLURM_NTASKS'])
                            ~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "<frozen os>", line 679, in __getitem__
KeyError: 'SLURM_NTASKS'
==============================================================================
Running epilogue script on indigo54.

Submit time  : 2025-03-08T11:42:43
Start time   : 2025-03-09T01:23:07
End time     : 2025-03-09T01:23:48
Elapsed time : 00:00:41 (Timelimit=1-00:00:00)

Job ID: 7151661
Cluster: i5
User/Group: as1g21/fp
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 32
CPU Utilized: 00:00:11
CPU Efficiency: 0.84% of 00:21:52 core-walltime
Job Wall-clock time: 00:00:41
Memory Utilized: 304.51 MB
Memory Efficiency: 0.00% of 16.00 B

